{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "\n",
    "# Import package/module for data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules for feature engineering and modelling\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# cross validation and hyperparameter tuning\n",
    "from sklearn.model_selection import StratifiedGroupKFold,  GridSearchCV\n",
    "\n",
    "# balancing\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "#accuracy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "#feature selection\n",
    "from sklearn import set_config\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = r\"C:\\Users\\Julia\\Desktop\\features_with_labels.csv\"\n",
    "#read_path = r\"W:\\WCT\\04_Mind-Wandering-Labstudy\\04_Daten\\04_Prepared_data\\00_Julia\\Model Building\\features_with_labels.csv\"\n",
    "df = pd.read_csv(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['task-related', 'aware', 'unaware'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Awareness_all\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task-related    631\n",
       "aware           249\n",
       "unaware         147\n",
       "Name: Awareness_all, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[\"Awareness_all\"].unique()\n",
    "df[\"Awareness_all\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "df[\"Awareness_all_new\"] = labelencoder.fit_transform(df[\"Awareness_all\"])\n",
    "df[\"Awareness_all_new\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    631\n",
       "0    249\n",
       "2    147\n",
       "Name: Awareness_all_new, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Awareness_all_new\"].value_counts()\n",
    "# 1 = task related, 0 aware 2 unaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['task-related' == 1 , 'aware' == 2, 'unaware' == 3 , nan == delete the row]\n",
    "def get_X_y(train):\n",
    "    FEATURES = [\n",
    "        'Fixation Duration Mean [ms]', 'Fixation Duration Max [ms]', 'Fixation Duration Min [ms]', 'Fixation Duration Median [ms]', 'Fixation Duration Std [ms]', 'Fixation Duration Skew [ms]', 'Fixation Duration Quantil 25 [ms]', 'Fixation Duration Quantil 75 [ms]',\n",
    "        'Saccade Duration Mean [ms]', 'Saccade Duration Max [ms]', 'Saccade Duration Min [ms]', 'Saccade Duration Median [ms]', 'Saccade Duration Std [ms]', 'Saccade Duration Skew [ms]', 'Saccade Duration Quantil 25 [ms]', 'Saccade Duration Quantil 75 [ms]', \n",
    "        'Blink Duration Mean [ms]', 'Blink Duration Max [ms]', 'Blink Duration Min [ms]', 'Blink Duration Median [ms]', 'Blink Duration Std [ms]', 'Blink Duration Skew [ms]', 'Blink Duration Quantil 25 [ms]', 'Blink Duration Quantil 75 [ms]', 'Fixation Duration Kurtosis [ms]',\n",
    "        'Saccade Duration Kurtosis [ms]',\n",
    "        'Blink Duration Kurtosis [ms]', \n",
    "        'Fixation Saccade Ratio Mean', 'Fixation Saccade Ratio Max', 'Fixation Saccade Ratio Min', 'Fixation Saccade Ratio Median', 'Fixation Saccade Ratio Std', 'Fixation Saccade Ratio Skew', 'Fixation Saccade Ratio Kurtosis', \n",
    "        'Fixation Number', 'Blink Number', \n",
    "        'Fixation Dispersion X Mean [px]', 'Fixation Dispersion X Max [px]', 'Fixation Dispersion X Min [px]', 'Fixation Dispersion X Median [px]', 'Fixation Dispersion X Std [px]', 'Fixation Dispersion X Skew [px]', 'Fixation Dispersion X Quantil 25 [px]', 'Fixation Dispersion X Quantil 75 [px]', \n",
    "        'Fixation Dispersion Y Mean [px]', 'Fixation Dispersion Y Max [px]', 'Fixation Dispersion Y Min [px]', 'Fixation Dispersion Y Median [px]', 'Fixation Dispersion Y Std [px]', 'Fixation Dispersion Y Skew [px]', 'Fixation Dispersion Y Quantil 25 [px]', 'Fixation Dispersion Y Quantil 75 [px]', 'Fixation Dispersion X Kurtosis [px]', 'Fixation Dispersion Y Kurtosis [px]', \n",
    "        'Saccade Amplitude Mean [°]', 'Saccade Amplitude Max [°]', 'Saccade Amplitude Min [°]', 'Saccade Amplitude Median [°]', 'Saccade Amplitude Std [°]', 'Saccade Amplitude Skew [°]', 'Saccade Amplitude Quantil 25 [°]', 'Saccade Amplitude Quantil 75 [°]', 'Saccade Amplitude Kurtosis [°]',\n",
    "        'Saccade Acceleration Average [°/s²] Mean', 'Saccade Acceleration Average [°/s²] Max', 'Saccade Acceleration Average [°/s²] Min', 'Saccade Acceleration Average [°/s²] Median', 'Saccade Acceleration Average [°/s²] Std', 'Saccade Acceleration Average [°/s²] Skew]', 'Saccade Acceleration Average [°/s²] Quantil 25]', 'Saccade Acceleration Average [°/s²] Quantil 75]',\n",
    "        'Saccade Acceleration Peak [°/s²] Mean', 'Saccade Acceleration Peak [°/s²] Max', 'Saccade Acceleration Peak [°/s²] Min', 'Saccade Acceleration Peak [°/s²] Median', 'Saccade Acceleration Peak [°/s²] Std', 'Saccade Acceleration Peak [°/s²] Skew]', 'Saccade Acceleration Peak [°/s²] Quantil 25]', 'Saccade Acceleration Peak [°/s²] Quantil 75]', 'Saccade Deceleration Peak [°/s²] Mean', \n",
    "        'Saccade Deceleration Peak [°/s²] Max', 'Saccade Deceleration Peak [°/s²] Min', 'Saccade Deceleration Peak [°/s²] Median', 'Saccade Deceleration Peak [°/s²] Std', 'Saccade Deceleration Peak [°/s²] Skew]', 'Saccade Deceleration Peak [°/s²] Quantil 25]', 'Saccade Deceleration Peak [°/s²] Quantil 75]', \n",
    "        'Saccade Velocity Average [°/s²] Mean', 'Saccade Velocity Average [°/s²] Max', 'Saccade Velocity Average [°/s²] Min', 'Saccade Velocity Average [°/s²] Median', 'Saccade Velocity Average [°/s²] Std', 'Saccade Velocity Average [°/s²] Skew]', 'Saccade Velocity Average [°/s²] Quantil 25]', 'Saccade Velocity Average [°/s²] Quantil 75]', \n",
    "        'Saccade Velocity Peak [°/s²] Mean', 'Saccade Velocity Peak [°/s²] Max', 'Saccade Velocity Peak [°/s²] Min', 'Saccade Velocity Peak [°/s²] Median', 'Saccade Velocity Peak [°/s²] Std', 'Saccade Velocity Peak [°/s²] Skew]', 'Saccade Velocity Peak [°/s²] Quantil 25]', 'Saccade Velocity Peak [°/s²] Quantil 75]', \n",
    "        'Saccade Velocity Peak [%] Mean', 'Saccade Velocity Peak [%] Max', 'Saccade Velocity Peak [%] Min', 'Saccade Velocity Peak [%] Median', 'Saccade Velocity Peak [%] Std', 'Saccade Velocity Peak [%] Skew]', 'Saccade Velocity Peak [%] Quantil 25]', 'Saccade Velocity Peak [%] Quantil 75]', \n",
    "        'Saccade Acceleration Average [°/s²] Kurtosis', 'Saccade Acceleration Peak [°/s²] Kurtosis', 'Saccade Deceleration Peak [°/s²] Kurtosis', 'Saccade Velocity Average [°/s²] Kurtosis', 'Saccade Velocity Peak [°/s²] Kurtosis', 'Saccade Velocity Peak [%] Kurtosis', \n",
    "        'Saccade Length Mean [px]', 'Saccade Length Max [px]', 'Saccade Length Min [px]', 'Saccade Length Median [px]', 'Saccade Length Std [px]', 'Saccade Length Skew [px]]', 'Saccade Length Quantil 25 [px]]', 'Saccade Length Quantil 75 [px]]', 'Saccade Length Kurtosis [px]', \n",
    "        'Fixation Average Pupil Diameter [mm] Mean', 'Fixation Average Pupil Diameter [mm] Max', 'Fixation Average Pupil Diameter [mm] Min', 'Fixation Average Pupil Diameter [mm] Median', 'Fixation Average Pupil Diameter [mm] Std', 'Fixation Average Pupil Diameter [mm] Skew', 'Fixation Average Pupil Diameter [mm] Quantil25', 'Fixation Average Pupil Diameter [mm] Quantil75',\n",
    "        'Fixation Average Pupil Diameter [mm] Kurtosis', \n",
    "        'Veregence Angles Mean [rad]', 'Veregence Angles Std [rad]', \n",
    "        'Pupil Distance Mean [px]', 'Pupil Distance Std [px]'\n",
    "    ]\n",
    "\n",
    "    GROUPS = \"Participant\"\n",
    "\n",
    "    TARGET = \"Awareness_all_new\"\n",
    "\n",
    "    X = train[FEATURES]\n",
    "    y = train[TARGET]\n",
    "    groups = train[GROUPS]\n",
    "    return X, y, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups = get_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### preprocessing with pipleline #####################\n",
    "imputer =  SimpleImputer(fill_value='missing')\n",
    "scaler = StandardScaler()\n",
    "# Balancing\n",
    "over = SMOTE(random_state= 27) #sampling_strategy=0.1\n",
    "\n",
    "\n",
    "# getuned\n",
    "# XGBoost\n",
    "#model = xgb.XGBClassifier(objective=\"multi:softmax\", random_state=42, colsample_bytree =1.0, max_depth = 6, n_estimators= 700, subsample = 1)\n",
    "\n",
    "# GaussianNB\n",
    "#model = GaussianNB(var_smoothing = 0.001873817422860383)\n",
    "\n",
    "# Random forrest classifier\n",
    "#model = RandomForestClassifier(random_state=0, bootstrap = True, max_depth = 25,  max_features = \"auto\", min_samples_leaf = 1, min_samples_split= 1, n_estimators = 50)\n",
    "\n",
    "# SVC\n",
    "#{'model__C': 0.6, 'model__gamma': 0.001, 'model__kernel': 'rbf'}\n",
    "#model = SVC(kernel=\"rbf\", C= 0.6, gamma = 0.001, probability=True)\n",
    "\n",
    "#model = SVC(kernel=\"linear\", C= 0.8, gamma = 0.2, probability=True)\n",
    "\n",
    "model = MLPClassifier()\n",
    "\n",
    "steps = [('imputer', imputer), ('scaler',scaler),('over', over), ('model', model)]\n",
    "pipe = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 0:  0.2958700449475357\n",
      "recall with labels  score for fold 0:  0.3\n",
      "precision with labels  score for fold 0:  0.305209539932876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 1:  0.2639311043566363\n",
      "recall with labels  score for fold 1:  0.27092168504382247\n",
      "precision with labels  score for fold 1:  0.26337038092523984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 2:  0.37805469752069293\n",
      "recall with labels  score for fold 2:  0.3996848739495798\n",
      "precision with labels  score for fold 2:  0.40482017055988134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 3:  0.3656106519742883\n",
      "recall with labels  score for fold 3:  0.3905122655122655\n",
      "precision with labels  score for fold 3:  0.3667772444946358\n",
      "f1 with labels  score for fold 4:  0.3297532499569851\n",
      "recall with labels  score for fold 4:  0.3527963291081732\n",
      "precision with labels  score for fold 4:  0.3266983448857114\n",
      "Our mean fold f1 score is 0.3266\n",
      "Our mean fold recall is 0.3428\n",
      "Our mean fold precisionis 0.3334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##################### prediction without baseline, macro f1 score #####################\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "f1_scores = []\n",
    "precision_scores =[]\n",
    "recall_scores =[]\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "### stratifies group k fold\n",
    "for train_index, test_index in sgk.split(X, y, groups):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    # Fit Model on Train\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    #accurancy_score = accuracy_score(y_test, y_pred)\n",
    "    labels = [0, 1, 2]\n",
    "    f1 = f1_score(y_test, y_pred, average= \"macro\", labels=labels)\n",
    "\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\", labels=labels)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\", labels=labels)      \n",
    "    \n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"f1 with labels  score for fold {fold}: \", f1)\n",
    "    print(f\"recall with labels  score for fold {fold}: \", recall)\n",
    "    print(f\"precision with labels  score for fold {fold}: \", precision)\n",
    "    fold += 1\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores)\n",
    "\n",
    "print(f'Our mean fold f1 score is {mean_f1:0.4f}')\n",
    "print(f'Our mean fold recall is {mean_recall:0.4f}')\n",
    "print(f'Our mean fold precisionis {mean_precision:0.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "{'model__colsample_bytree': 1.0, 'model__max_depth': 6, 'model__n_estimators': 700, 'model__subsample': 1}\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()), ('over', SMOTE(random_state=27)),\n",
      "                ('model',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma...\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=700,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               objective='multi:softprob', predictor=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "### Grid Seach Gaussian xg boost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {\n",
    "    \"model__subsample\": [0.6, 0.8, 1], #np.arange(0.6,1,0.05),\n",
    "    \"model__max_depth\": [3, 6, 10], # np.arange(3,10,1),\n",
    "    \"model__n_estimators\": [1000, 700, 500],\n",
    "    \"model__colsample_bytree\": [0.1,0.5, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid = param_grid, verbose = 1, cv = 5, scoring= \"f1_macro\")\n",
    "\n",
    "# muss hier mit trainig data oder mit ganzem X trainiert werden?\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'model__var_smoothing': 0.001873817422860383}\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()), ('over', SMOTE(random_state=27)),\n",
      "                ('model', GaussianNB(var_smoothing=0.001873817422860383))])\n"
     ]
    }
   ],
   "source": [
    "### Grid Seach Gaussian naive bayes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {\n",
    "    #var_smoothing is a stability calculation to widen (or smooth) the curve and therefore account for more samples that are further away from the distribution mean.\n",
    "    'model__var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid = param_grid, verbose = 1, cv = 5, scoring= \"f1_macro\")\n",
    "\n",
    "# muss hier mit trainig data oder mit ganzem X trainiert werden?\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__bootstrap': True, 'model__max_depth': 20, 'model__max_features': 'auto', 'model__min_samples_leaf': 1, 'model__min_samples_split': 1, 'model__n_estimators': 50}\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()), ('over', SMOTE(random_state=27)),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(max_depth=20, max_features='auto',\n",
      "                                        min_samples_split=1, n_estimators=50,\n",
      "                                        random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "###### Grid Search random forrest\n",
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#{'model__bootstrap': True, 'model__max_depth': 50, 'model__max_features': 'auto', 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n",
    "\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "# defining parameter range\n",
    "param_grid = {\n",
    "    'model__bootstrap': [True],\n",
    " 'model__max_depth': [25, 20, 30],\n",
    " 'model__max_features': ['auto'],\n",
    " 'model__min_samples_leaf': [1],\n",
    " 'model__min_samples_split': [1, 2],\n",
    " 'model__n_estimators': [50, 100]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid = param_grid, verbose = 1, cv = 5, scoring= \"f1_macro\")\n",
    "\n",
    "grid.fit(X, y)\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "{'model__C': 0.6, 'model__gamma': 0.001, 'model__kernel': 'rbf'}\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()), ('over', SMOTE(random_state=27)),\n",
      "                ('model', SVC(C=0.6, gamma=0.001, probability=True))])\n"
     ]
    }
   ],
   "source": [
    "############## Grid Search SVC\n",
    "##################### prediction without baseline #####################\n",
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "# defining parameter range\n",
    "param_grid = {'model__C': [ 0.7, 0.5, 0.6], \n",
    "              'model__gamma':  [ 0.001, 0.01 , 0.005],\n",
    "              'model__kernel': [\"linear\", \"rbf\"]} \n",
    "  \n",
    "grid = GridSearchCV(pipe, param_grid = param_grid, verbose = 1, cv = 5, scoring= \"f1_macro\")\n",
    "\n",
    "grid.fit(X, y)\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "#'model__C': 0.8, 'model__gamma': 0.2, 'model__kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 0:  {0: 0.19047619047619047, 1: 0.5779467680608366, 2: 0.125}\n",
      "recall with labels  score for fold 0:  {0: 0.15873015873015872, 1: 0.6031746031746031, 2: 0.14285714285714285}\n",
      "precision with labels  score for fold 0:  {0: 0.23809523809523808, 1: 0.5547445255474452, 2: 0.1111111111111111}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 1:  {0: 0.24242424242424246, 1: 0.672, 2: 0.05128205128205128}\n",
      "recall with labels  score for fold 1:  {0: 0.3333333333333333, 1: 0.6412213740458015, 2: 0.037037037037037035}\n",
      "precision with labels  score for fold 1:  {0: 0.19047619047619047, 1: 0.7058823529411765, 2: 0.08333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 2:  {0: 0.40287769784172667, 1: 0.6161137440758293, 2: 0.1}\n",
      "recall with labels  score for fold 2:  {0: 0.5833333333333334, 1: 0.5462184873949579, 2: 0.07142857142857142}\n",
      "precision with labels  score for fold 2:  {0: 0.3076923076923077, 1: 0.7065217391304348, 2: 0.16666666666666666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 with labels  score for fold 3:  {0: 0.3650793650793651, 1: 0.6115702479338843, 2: 0.041666666666666664}\n",
      "recall with labels  score for fold 3:  {0: 0.5227272727272727, 1: 0.5285714285714286, 2: 0.041666666666666664}\n",
      "precision with labels  score for fold 3:  {0: 0.2804878048780488, 1: 0.7254901960784313, 2: 0.041666666666666664}\n",
      "f1 with labels  score for fold 4:  {0: 0.338235294117647, 1: 0.5726872246696035, 2: 0.12244897959183672}\n",
      "recall with labels  score for fold 4:  {0: 0.39655172413793105, 1: 0.5652173913043478, 2: 0.09090909090909091}\n",
      "precision with labels  score for fold 4:  {0: 0.2948717948717949, 1: 0.5803571428571429, 2: 0.1875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##################### prediction for each class without baseline #####################\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "f1_scores ={\"0\": [], \"1\": [], \"2\": []}\n",
    "precision_scores = {\"0\": [], \"1\": [], \"2\": []}\n",
    "recall_scores ={\"0\": [], \"1\": [], \"2\": []}\n",
    "\n",
    "### stratifies group k fold\n",
    "for train_index, test_index in sgk.split(X, y, groups):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    # Fit Model on Train\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    #accurancy_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    labels = [0, 1, 2]\n",
    "    f1 = f1_score(y_test, y_pred, average= None, labels=labels)\n",
    "    f1_scores[\"0\"].append(f1[0])\n",
    "    f1_scores[\"1\"].append(f1[1])\n",
    "    f1_scores[\"2\"].append(f1[2])\n",
    "  \n",
    "\n",
    "    recall = recall_score(y_test, y_pred, average=None, labels=labels)\n",
    "    recall_scores[\"0\"].append(recall[0])\n",
    "    recall_scores[\"1\"].append(recall[1])\n",
    "    recall_scores[\"2\"].append(recall[2])\n",
    "    #print(recall)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=None, labels=labels)\n",
    "    precision_scores[\"0\"].append(precision[0])\n",
    "    precision_scores[\"1\"].append(precision[1])\n",
    "    precision_scores[\"2\"].append(precision[2])\n",
    "    #print(recall)\n",
    "\n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "    #auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    #print(f\"f1 with labels  score for fold {fold}: \", auc)\n",
    "    print(f\"f1 with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, f1)})\n",
    "    print(f\"recall with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, recall)})\n",
    "    print(f\"precision with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, precision)})\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# # 1 = task related, 0 aware 2 unaware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: [('0', 0.30781855798783436), ('1', 0.6100635969480307), ('2', 0.08807953950811094)]\n",
      "precision score:  [('0', 0.262324667202716), ('1', 0.6545991913109261), ('2', 0.11805555555555554)]\n",
      "recall: [('0', 0.39893516445240584), ('1', 0.5768806568982277), ('2', 0.07677970177970177)]\n"
     ]
    }
   ],
   "source": [
    "averages = [(k, sum(v)/len(v)) for k, v in f1_scores.items()]\n",
    "print(\"f1 score:\", averages)\n",
    "# for k, v in averages:\n",
    "#     print(\"f1 score, label\", k, f\"\\n{v}\")\n",
    "\n",
    "averages = [(k, sum(v)/len(v)) for k, v in precision_scores.items()]\n",
    "print(\"precision score: \", averages)\n",
    "# for k, v in averages:\n",
    "#     print(\"precision score, label\", k, f\"\\n{v}\")\n",
    "\n",
    "\n",
    "averages = [(k, sum(v)/len(v)) for k, v in recall_scores.items()]\n",
    "print(\"recall:\", averages)\n",
    "# for k, v in averages:\n",
    "#     print(\"recall score, label\", k, f\"\\n{v}\")\n",
    "\n",
    "# # 1 = task related, 0 aware 2 unaware\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# logistic_fpr, logistic_tpr, threshold = roc_curve(y_test, y_pred_logistic)\n",
    "# auc_logistic = auc(logistic_fpr, logistic_tpr)\n",
    "\n",
    "# svm_fpr, svm_tpr, threshold = roc_curve(y_test, y_pred_svm)\n",
    "# auc_svm = auc(svm_fpr, svm_tpr)\n",
    "\n",
    "# plt.figure(figsize=(5, 5), dpi=100)\n",
    "# plt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)\n",
    "# plt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\n",
    "\n",
    "# plt.xlabel('False Positive Rate -->')\n",
    "# plt.ylabel('True Positive Rate -->')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # one vs. rest\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# micro_roc_auc_ovr = roc_auc_score(\n",
    "#     y_test,\n",
    "#     y_score,\n",
    "#     multi_class=\"ovr\",\n",
    "#     average=\"micro\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Macro F1 score: prediction with baseline #####################\n",
    "# task-related    631\n",
    "# aware           249\n",
    "# unaware         147\n",
    "\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "\n",
    "fold = 0\n",
    "f1_scores =[]\n",
    "precision_scores =[]\n",
    "recall_scores =[]\n",
    "\n",
    "### stratifies group k fold\n",
    "for i in range(10):\n",
    "    for train_index, test_index in sgk.split(X, y, groups):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "        # Fit Model on Train\n",
    "        pipe.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = pipe.predict(X_test)\n",
    "    \n",
    "        # create baseline\n",
    "        baseline = np.ones(len(y_pred))\n",
    "        mw__aware_size = 24.24537/100  * len(y_pred)\n",
    "        mw__unaware_size = 14.31353/100  * len(y_pred)\n",
    "    \n",
    "        baseline[:int(mw__aware_size)] = 0\n",
    "        baseline[int(mw__aware_size): int(mw__aware_size) + int(mw__unaware_size)] = 2\n",
    "        np.random.shuffle(baseline)\n",
    "        baseline = baseline.astype(int)\n",
    "    \n",
    "        labels = [0, 1, 2]\n",
    "        f1 = f1_score(y_test, baseline, average= \"macro\", labels=labels)\n",
    "        recall = recall_score(y_test, baseline, average=\"macro\", labels=labels)\n",
    "        precision = precision_score(y_test, baseline, average=\"macro\", labels=labels)\n",
    "\n",
    "        # fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "        # auc_score= auc(fpr, tpr)\n",
    "\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "   #     auc_scores.append(auc_score)\n",
    "\n",
    "\n",
    "\n",
    "        #print(f\"f1 with labels  score for fold {fold}: \", auc_score)\n",
    "        print(f\"f1 with labels  score for fold {fold}: \", f1)\n",
    "        print(f\"recall with labels  score for fold {fold}: \", recall)\n",
    "        print(f\"precision with labels  score for fold {fold}: \", precision)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores)\n",
    "\n",
    "#mean_auc = np.mean(auc_scores)\n",
    "\n",
    "print(f'Our mean fold f1 score is {mean_f1:0.4f}')\n",
    "print(f'Our mean fold recall is {mean_recall:0.4f}')\n",
    "print(f'Our mean fold precisionis {mean_precision:0.4f}')\n",
    "\n",
    "#print(f'Our mean fold auc {mean_auc:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### each calss prediction with baseline #####################\n",
    "# task-related    631\n",
    "# aware           249\n",
    "# unaware         147\n",
    "\n",
    "import random\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "\n",
    "fold = 0\n",
    "f1_scores ={\"0\": [], \"1\": [], \"2\": []}\n",
    "precision_scores = {\"0\": [], \"1\": [], \"2\": []}\n",
    "recall_scores ={\"0\": [], \"1\": [], \"2\": []}\n",
    "\n",
    "### stratifies group k fold\n",
    "for i in range(10):\n",
    "    for train_index, test_index in sgk.split(X, y, groups):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "        # Fit Model on Train\n",
    "        pipe.fit(X_train, y_train)\n",
    "    \n",
    "        y_pred = pipe.predict(X_test)\n",
    "    \n",
    "        # create baseline\n",
    "        baseline = np.ones(len(y_pred))\n",
    "        mw__aware_size = 24.24537/100  * len(y_pred)\n",
    "        mw__unaware_size = 14.31353/100  * len(y_pred)\n",
    "    \n",
    "        baseline[:int(mw__aware_size)] = 0\n",
    "        baseline[int(mw__aware_size): int(mw__aware_size) + int(mw__unaware_size)] = 2\n",
    "        np.random.shuffle(baseline)\n",
    "        baseline = baseline.astype(int)\n",
    "\n",
    "    \n",
    "        labels = [0, 1, 2]\n",
    "        f1 = f1_score(y_test, baseline, average=None, labels=labels)\n",
    "        f1_scores[\"0\"].append(f1[0])\n",
    "        f1_scores[\"1\"].append(f1[1])\n",
    "        f1_scores[\"2\"].append(f1[2])\n",
    "        print(f1)\n",
    "    \n",
    "        recall = recall_score(y_test, baseline, average=None, labels=labels)\n",
    "        recall_scores[\"0\"].append(recall[0])\n",
    "        recall_scores[\"1\"].append(recall[1])\n",
    "        recall_scores[\"2\"].append(recall[2])\n",
    "        #print(recall)\n",
    "    \n",
    "        precision = precision_score(y_test, baseline, average=None, labels=labels)\n",
    "        precision_scores[\"0\"].append(precision[0])\n",
    "        precision_scores[\"1\"].append(precision[1])\n",
    "        precision_scores[\"2\"].append(precision[2])\n",
    "        #print(recall)\n",
    "    \n",
    "        #fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "        #auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "        #print(f\"f1 with labels  score for fold {fold}: \", auc)\n",
    "        print(f\"f1 with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, f1)})\n",
    "        print(f\"recall with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, recall)})\n",
    "        print(f\"precision with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, precision)})\n",
    "        \n",
    "        fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: [('0', 0.24422749932144583), ('1', 0.6154369720912743), ('2', 0.1337215297569219)]\n",
      "precision: [('0', 0.24554979816985903), ('1', 0.6150007595603985), ('2', 0.13507104086845467)]\n",
      "recall: [('0', 0.24628501766432806), ('1', 0.6179746758754394), ('2', 0.1332135642135642)]\n"
     ]
    }
   ],
   "source": [
    "f1_averages = [(k, sum(v)/len(v)) for k, v in f1_scores.items()]\n",
    "print(\"f1 score:\",f1_averages)\n",
    "\n",
    "precision_averages = [(k, sum(v)/len(v)) for k, v in precision_scores.items()]\n",
    "print(\"precision:\", precision_averages)\n",
    "\n",
    "recall_averages = [(k, sum(v)/len(v)) for k, v in recall_scores.items()]\n",
    "print(\"recall:\", recall_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### macro f1 score preprocessing with pipleline #####################\n",
    "imputer =  SimpleImputer(fill_value='missing')\n",
    "scaler = StandardScaler()\n",
    "over = SMOTE(random_state= 27) #sampling_strategy=0.1\n",
    "\n",
    "# optimized XGBoost\n",
    "model = xgb.XGBClassifier(objective=\"objective=multi:softmax\", random_state=42, colsample_bytree =1.0, max_depth = 6, n_estimators= 700, subsample = 1, scale_pos_weight = 10)\n",
    "\n",
    "# optimzied GaussianNB\n",
    "#model = GaussianNB(var_smoothing = 0.001873817422860383)\n",
    "\n",
    "\n",
    "# optimzied Random forrest classifier\n",
    "#model = RandomForestClassifier(random_state=0, bootstrap = True, max_depth = 25,  max_features = \"auto\", min_samples_leaf = 1, min_samples_split= 1, n_estimators = 50, class_weight=\"balanced\")\n",
    "\n",
    "# optimized SVC\n",
    "#{'model__C': 0.6, 'model__gamma': 0.001, 'model__kernel': 'rbf'}\n",
    "#model = SVC(kernel=\"rbf\", C= 0.6, gamma = 0.001, probability=True, class_weight='balanced')\n",
    "\n",
    "#model = SVC(kernel=\"linear\", C= 0.8, gamma = 0.2, probability=True)\n",
    "\n",
    "#steps = [('imputer', imputer), ('scaler',scaler),('over', over), ('model', model)]\n",
    "# with weights\n",
    "steps = [('imputer', imputer), ('scaler',scaler), ('model', model)]\n",
    "\n",
    "pipe = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:11:03] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "f1 with labels  score for fold 0:  0.26095516326234586\n",
      "recall with labels  score for fold 0:  0.3296296296296296\n",
      "precision with labels  score for fold 0:  0.33900625978090765\n",
      "[16:11:06] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "f1 with labels  score for fold 1:  0.293519887764001\n",
      "recall with labels  score for fold 1:  0.3194797851286401\n",
      "precision with labels  score for fold 1:  0.27952790072029854\n",
      "[16:11:08] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "f1 with labels  score for fold 2:  0.3199163932932385\n",
      "recall with labels  score for fold 2:  0.35194911297852477\n",
      "precision with labels  score for fold 2:  0.4217171717171717\n",
      "[16:11:10] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "f1 with labels  score for fold 3:  0.2964858955900389\n",
      "recall with labels  score for fold 3:  0.32987012987012987\n",
      "precision with labels  score for fold 3:  0.3044363044363045\n",
      "[16:11:13] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "f1 with labels  score for fold 4:  0.28970276731470757\n",
      "recall with labels  score for fold 4:  0.34473975133645296\n",
      "precision with labels  score for fold 4:  0.4272692381500671\n",
      "Our mean fold f1 score is 0.2921\n",
      "Our mean fold recall is 0.3351\n",
      "Our mean fold precisionis 0.3544\n"
     ]
    }
   ],
   "source": [
    "############# Balancing with weights naive bayes\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "##################### prediction without baseline, macro f1 score #####################\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import  auc\n",
    "\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "f1_scores = []\n",
    "precision_scores =[]\n",
    "recall_scores =[]\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "### stratifies group k fold\n",
    "for train_index, test_index in sgk.split(X, y, groups):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    # sample weights\n",
    "    sample_weights = class_weight.compute_sample_weight(class_weight = 'balanced',  y = y_train)\n",
    "\n",
    "    # Fit Model on Train\n",
    "    #pipe.fit(X_train, y_train, **{'model__sample_weight': sample_weights})\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    #accurancy_score = accuracy_score(y_test, y_pred)\n",
    "    labels = [0, 1, 2]\n",
    "    f1 = f1_score(y_test, y_pred, average= \"macro\", labels=labels)\n",
    "\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\", labels=labels)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\", labels=labels)      \n",
    "    \n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"f1 with labels  score for fold {fold}: \", f1)\n",
    "    print(f\"recall with labels  score for fold {fold}: \", recall)\n",
    "    print(f\"precision with labels  score for fold {fold}: \", precision)\n",
    "    fold += 1\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores)\n",
    "\n",
    "print(f'Our mean fold f1 score is {mean_f1:0.4f}')\n",
    "print(f'Our mean fold recall is {mean_recall:0.4f}')\n",
    "print(f'Our mean fold precisionis {mean_precision:0.4f}')\n",
    "\n",
    "\n",
    "#Our mean fold f1 score is 0.2788\n",
    "# Our mean fold recall is 0.3509\n",
    "# Our mean fold precisionis 0.3484\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:19:40] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=700,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               objective='multi:softprob', predictor=None, ...))])\n",
      "f1 with labels  score for fold 0:  {0: 0.049999999999999996, 1: 0.6493506493506493, 2: 0.03333333333333333}\n",
      "recall with labels  score for fold 0:  {0: 0.031746031746031744, 1: 0.7936507936507936, 2: 0.02857142857142857}\n",
      "precision with labels  score for fold 0:  {0: 0.11764705882352941, 1: 0.5494505494505495, 2: 0.04}\n",
      "[16:19:42] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=700,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               objective='multi:softprob', predictor=None, ...))])\n",
      "f1 with labels  score for fold 1:  {0: 0.2105263157894737, 1: 0.7686832740213523, 2: 0.0}\n",
      "recall with labels  score for fold 1:  {0: 0.2222222222222222, 1: 0.8244274809160306, 2: 0.0}\n",
      "precision with labels  score for fold 1:  {0: 0.2, 1: 0.72, 2: 0.0}\n",
      "[16:19:44] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=700,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               objective='multi:softprob', predictor=None, ...))])\n",
      "f1 with labels  score for fold 2:  {0: 0.4369747899159664, 1: 0.7302904564315353, 2: 0.13333333333333333}\n",
      "recall with labels  score for fold 2:  {0: 0.5416666666666666, 1: 0.7394957983193278, 2: 0.07142857142857142}\n",
      "precision with labels  score for fold 2:  {0: 0.36619718309859156, 1: 0.7213114754098361, 2: 1.0}\n",
      "[16:19:46] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=700,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               objective='multi:softprob', predictor=None, ...))])\n",
      "f1 with labels  score for fold 3:  {0: 0.4, 1: 0.7913669064748201, 2: 0.06060606060606061}\n",
      "recall with labels  score for fold 3:  {0: 0.4772727272727273, 1: 0.7857142857142857, 2: 0.041666666666666664}\n",
      "precision with labels  score for fold 3:  {0: 0.3442622950819672, 1: 0.7971014492753623, 2: 0.1111111111111111}\n",
      "[16:19:48] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "Pipeline(steps=[('imputer', SimpleImputer(fill_value='missing')),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=700,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               objective='multi:softprob', predictor=None, ...))])\n",
      "f1 with labels  score for fold 4:  {0: 0.3793103448275862, 1: 0.682170542635659, 2: 0.10526315789473685}\n",
      "recall with labels  score for fold 4:  {0: 0.3793103448275862, 1: 0.7652173913043478, 2: 0.06060606060606061}\n",
      "precision with labels  score for fold 4:  {0: 0.3793103448275862, 1: 0.6153846153846154, 2: 0.4}\n"
     ]
    }
   ],
   "source": [
    "##################### prediction for each class without baseline #####################\n",
    "sgk = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=529)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "f1_scores ={\"0\": [], \"1\": [], \"2\": []}\n",
    "precision_scores = {\"0\": [], \"1\": [], \"2\": []}\n",
    "recall_scores ={\"0\": [], \"1\": [], \"2\": []}\n",
    "\n",
    "### stratifies group k fold\n",
    "for train_index, test_index in sgk.split(X, y, groups):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    # # sample weights\n",
    "    sample_weights = class_weight.compute_sample_weight(class_weight = 'balanced',  y = y_train)\n",
    "    # Fit Model on Train\n",
    "    pipe.fit(X_train, y_train, **{'model__sample_weight': sample_weights})\n",
    "    print(pipe)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    #accurancy_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    labels = [0, 1, 2]\n",
    "    f1 = f1_score(y_test, y_pred, average= None, labels=labels)\n",
    "    f1_scores[\"0\"].append(f1[0])\n",
    "    f1_scores[\"1\"].append(f1[1])\n",
    "    f1_scores[\"2\"].append(f1[2])\n",
    "  \n",
    "\n",
    "    recall = recall_score(y_test, y_pred, average=None, labels=labels)\n",
    "    recall_scores[\"0\"].append(recall[0])\n",
    "    recall_scores[\"1\"].append(recall[1])\n",
    "    recall_scores[\"2\"].append(recall[2])\n",
    "    #print(recall)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=None, labels=labels)\n",
    "    precision_scores[\"0\"].append(precision[0])\n",
    "    precision_scores[\"1\"].append(precision[1])\n",
    "    precision_scores[\"2\"].append(precision[2])\n",
    "    #print(recall)\n",
    "\n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "    #auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    #print(f\"f1 with labels  score for fold {fold}: \", auc)\n",
    "    print(f\"f1 with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, f1)})\n",
    "    print(f\"recall with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, recall)})\n",
    "    print(f\"precision with labels  score for fold {fold}: \", {label:score for label,score in zip(labels, precision)})\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# # 1 = task related, 0 aware 2 unaware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: [('0', 0.007692307692307693), ('1', 0.0), ('2', 0.2469730077037009)]\n",
      "precision score:  [('0', 0.05), ('1', 0.0), ('2', 0.14153013427371228)]\n",
      "recall: [('0', 0.004166666666666667), ('1', 0.0), ('2', 0.9800000000000001)]\n"
     ]
    }
   ],
   "source": [
    "averages = [(k, sum(v)/len(v)) for k, v in f1_scores.items()]\n",
    "print(\"f1 score:\", averages)\n",
    "\n",
    "averages = [(k, sum(v)/len(v)) for k, v in precision_scores.items()]\n",
    "print(\"precision score: \", averages)\n",
    "\n",
    "averages = [(k, sum(v)/len(v)) for k, v in recall_scores.items()]\n",
    "print(\"recall:\", averages)\n",
    "\n",
    "\n",
    "# # 1 = task related, 0 aware 2 unaware\n",
    "\n",
    "# f1 score: [('0', 0.35398526125547203), ('1', 0.3711727446863309), ('2', 0.1209414698776401)]\n",
    "# precision score:  [('0', 0.25211613582691406), ('1', 0.6135062978288349), ('2', 0.16936507936507936)]\n",
    "# recall: [('0', 0.6506636562671047), ('1', 0.2939542716079683), ('2', 0.09741702741702742)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# pipeline\n",
    "# wieghts must be added in the fit step\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"NLP\", TfidfVectorizer(),\n",
    "                           (\"MNB\", MultinomialNB())\n",
    "                          ])\n",
    "pipeline.fit(X_train, \n",
    "             y_train, \n",
    "             **{'MNB__sample_weight': sample_weights})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a166e61b3f1e7b9ccc9f59f78d7cc3087a9ccbb0b1a7cff444b301613a633a54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
